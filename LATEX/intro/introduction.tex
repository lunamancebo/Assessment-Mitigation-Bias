With the increased use of new technologies and algorithms in sensible tasks such as hate speech detection or predictive policing, it is crucial that not only the algorithm has a high performance, but also that the data which it is trained with is fair and represents every population group. As stated in FRA 2022 \cite{FRA2022} ``algorithms are only as good as the data that are used to develop them''. 

The term 'bias' can have different meanings depending on the field of study. Bias can refer to any of the following:
\begin{itemize}
    \item Differential treatment based on protected characteristics: this refers to an inclination toward a certain group of people based on characteristics such as gender, ethnicity, religion...
    \item Statistical bias: it exists when the data isn't adequately measuring what they are intended to measure. For example, if a sample of the general population contains more men than women, it is said to be biased towards men.
\end{itemize}
Biases in algorithmic systems may lead to discrimination, but it is important to differentiate between the two terms. Not all forms of bias relate to protected characteristics. For instance, an algorithmic model that differentiates between people based on whether or not they have pets, doesn't target a protected characteristics. Moreover, even if the target is a protected characteristic, if the result doesn't lead to a disadvantageous situation for a group of people, it isn't considered discriminatory. Therefore the problem arises when bias in algorithms results in direct discrimination when the target is a protected characteristic and it leads to a less favourable treatment for a certain group of people. 

Bias most frequently occurs when the data used to train an algorithm only reflect certain demographic groups or reflect social biases. Determining where bias comes from is a challenging exercise, and as it is a fairly new subject, many studies are being made on this matter. How bias can be detected and eliminated in AI applications are at the center of discussions around regulating AI. Unified regulations and policies regarding this subject are crucial in order to eradicate bias from algorithms, so that we can all benefit, without discrimination, from such a powerful tool. 

EU institutions have become more engaged in the area of AI, bias and fundamental rights, since they have acknowledged it is a problem that needs mitigation through policy and legislative proposals. Since 2017, institutions such as the European Parliament, the European Council and the European Commission, have been working towards a proposal for regulating AI applications. On 21 April of 2021, the Commission published its proposal for an \acrshort{AIA}, which forms part of its digital strategy, and is a key aspect for the EU to make its law fit for the digital age.

Not only researchers or policymakers are interested on the subject of bias in AI, the press are very sceptical about these algorithms, and if it is possible they could be making more harm than good. For instance, several articles \cite{jorge2021veripol,pikaramagazine} are being written stating that the AI VeriPol, whose goal is to predict if a police report is fake or not, is biased because of the differences in writing styles depending on the complainant's demography. These statements are subjective, as no studies have been made that demonstrate that differences in writing styles exist depending on where the authors of the text are from. Even if these claims where true, and differences in writing styles existed based on the demography of the author, it hasn't been proven that the prediction of the algorithm is less favourable towards a group of people. Therefore it is extremely important to conduct studies that uncover bias, in order to eliminate it, or on the other hand that demonstrate that bias doesn't exist and silence false claims. 

\acrshort{NLP} refers to the branch of computer science, more specifically the branch of \acrshort{AI}, concerned with giving computers the ability to understand text and spoken words. It is a field that is rapidly growing and many progress has been made recently. With it, a new area susceptible to bias has opened. Algorithms that predict hate speech or sentiment analysis, for instance, have been proven to be biased \cite{thelwall2018gender,kiritchenko2018examining} and many studies have been conducted in order to tackle this problem, building up methods to mitigate bias \cite{dayanik2021disentangling,diaz2018addressing,deshpande2020mitigating}. Other studies have worked on the task of creating a framework to detect bias in text \cite{asyrofi2021biasfinder,feder2020active}. 

The main goal of this study is to extensively analyze the differences in writing styles, lexical and syntactical, depending on the author's demographics. In this paper we don't just focus on demographic traits like gender and age, but introduce the novelty of studying if differences in writing styles exist depending on the geographic region of the author of a text. 

As social media is the most common used tool to express emotions and thoughts, most of the studies conducted on the field of extracting differences in writing styles based on demographic traits have used datasets containing texts of social media platforms like Twitter. The second point of study of this paper is, using the same demographic characteristics of gender, age and region for extracting writing style differences, compare the obtained results when using a dataset of informal texts (i.e tweets), or formal texts (i.e police reports). 

If the results obtained for the police dataset are statistically significant, which would mean differences in writing style exist, the next point to tackle in this paper would be to study whether or not the algorithm VeriPol is less favourable towards a specific group. In case it was discriminatory, the final posed question for this research would be: what techniques can be used to eliminate the existing bias? Are they useful? Do they mitigate bias, or is the algorithm still discriminatory?

The paper is organized as follows: in chapter \ref{CAP:STATEOFTHEART} we present the state of the art and summarize the most relevant findings of the bibliography research. 