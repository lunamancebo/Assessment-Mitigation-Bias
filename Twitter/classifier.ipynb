{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from emoji import emoji_count\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import string\n",
    "import statistics as stat\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.corpus import words as english_dict\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers and perform classifier only with most relevant features (those whose statistical measure is above 0.2)\n",
    "\n",
    "Features used:\n",
    "1. Twitter features\n",
    "2. Stylistic features (each of them by themselves and all together)\n",
    "3. N-gram features\n",
    "4. TF + SF\n",
    "5. TF + N-Gram\n",
    "6. SF + N-Gram\n",
    "7. TF + SF + N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data,outliers):\n",
    "    for column,value in zip(outliers.keys(),outliers.values()):\n",
    "        outliers_index = np.where(data[column] > value)\n",
    "        data.drop(outliers_index[0], inplace=True)\n",
    "        data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    x = data.values\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(classifier, data, features, target, outliers=None, type='binomial', normalize=True, ngram=False):\n",
    "    aux = features + [target]\n",
    "    data.drop([f for f in data.columns if f not in aux], axis=1, inplace=True)\n",
    "    if outliers != None:\n",
    "        data = remove_outliers(data,outliers)\n",
    "    \n",
    "    # obtain target variable\n",
    "    map_dict = {}\n",
    "    i = 0\n",
    "    for t in data[target].unique():\n",
    "        map_dict[t] = i\n",
    "        i += 1\n",
    "    \n",
    "    Y = list(data[target].map(map_dict))\n",
    "    data.drop([target], axis=1, inplace=True)\n",
    "\n",
    "    # get features\n",
    "    if ngram:\n",
    "        X = []\n",
    "        if len(features) > 1:\n",
    "            if ('wordgram' and 'chargram') in features:\n",
    "                for w,c in zip(data['wordgram'],data['chargram']):\n",
    "                    aux = w.tolist() + c.tolist()\n",
    "                    X.append(aux)\n",
    "            # for the tests of combination of features\n",
    "            else:\n",
    "                aux = []\n",
    "                for f in data['wordgram']:\n",
    "                    aux.append(f.tolist())\n",
    "                data.drop('wordgram',axis=1,inplace=True)\n",
    "                \n",
    "                data = data_scaler(data)\n",
    "                for list1,list2 in zip(aux,data.values.tolist()):\n",
    "                    X.append(list1+list2)\n",
    "        else:\n",
    "            for f in data[features[0]]:\n",
    "                X.append(f.tolist())\n",
    "\n",
    "    else:\n",
    "        data = data_scaler(data)\n",
    "        X = data.values.tolist()\n",
    "\n",
    "    # separate in train and test\n",
    "    X, Y = shuffle(X,Y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3,random_state=109)\n",
    "\n",
    "    if classifier == 'svm':\n",
    "        # Linear Kernel\n",
    "        clf = svm.SVC() \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        with open('results.txt','a') as f:\n",
    "            f.write('\\n\\t\\t* Without tunning')\n",
    "            f.write(f'\\n\\t\\t\\t-> Accuracy: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "            if classifier == 'binomial':\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred)}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred)}')\n",
    "            else:\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred,average=\"weighted\")}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred,average=\"weighted\")}')\n",
    "\n",
    "\n",
    "        # with hyperparameter tunning\n",
    "        # hyperparameter tuning\n",
    "        params = \"'kernel': 'rbf'\"\n",
    "        best_clf = svm.SVC(kernel='rbf') \n",
    "    \n",
    "    elif classifier == 'xgboost':\n",
    "        if type == 'binomial':\n",
    "            objective = 'binary:logistic'\n",
    "            clf = xgb.XGBRFClassifier(objective=objective)\n",
    "            params = \"{'learning_rate': 0.01, 'n_estimators': 200, 'reg_lambda': 1}\"\n",
    "        else:\n",
    "            objective = 'multi:softmax'\n",
    "            clf = xgb.XGBRFClassifier(objective=objective)\n",
    "            params = \"{'learning_rate': 0.01, 'n_estimators': 200, 'reg_lambda': 1}\"\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # with hyperparameter tunning\n",
    "        # hyperparameter tuning\n",
    "        best_clf = xgb.XGBRFClassifier(objective=objective,learning_rate=0.01,n_estimators=200,reg_lambda=1)\n",
    "        \n",
    "        with open('results.txt','a') as f:\n",
    "            f.write(f'\\n\\t\\t* Without tunning:')\n",
    "            f.write(f'\\n\\t\\t\\t-> Accuracy: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "            if classifier == 'binomial':\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred)}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred)}')\n",
    "            else:\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred,average=\"weighted\")}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred,average=\"weighted\")}')\n",
    "\n",
    "    \n",
    "\n",
    "    elif classifier == 'RF':\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_train,y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "\n",
    "        params = \"{'criterion': 'entropy', 'max_leaf_nodes': None, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 200}\"\n",
    "\n",
    "        best_clf = RandomForestClassifier(criterion='entropy',max_leaf_nodes=None,min_samples_leaf=4,min_samples_split=4,n_estimators=200)\n",
    "        \n",
    "        with open('results.txt','a') as f:\n",
    "            f.write(f'\\n\\t\\t* Without tunning:')\n",
    "            f.write(f'\\n\\t\\t\\t-> Accuracy: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "            if classifier == 'binomial':\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred)}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred)}')\n",
    "            else:\n",
    "                f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred,average=\"weighted\")}')\n",
    "                f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred,average=\"weighted\")}')\n",
    "\n",
    "    best_clf.fit(X_train, y_train)\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    with open('results.txt','a') as f:\n",
    "        f.write(f'\\n\\t\\t* Tunning:{params}')\n",
    "        f.write(f'\\n\\t\\t\\t-> Accuracy: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "        if classifier == 'binomial':\n",
    "            f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred)}')\n",
    "            f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred)}')\n",
    "        else:\n",
    "            f.write(f'\\n\\t\\t\\t-> Precision: {metrics.precision_score(y_test, y_pred,average=\"weighted\")}')\n",
    "            f.write(f'\\n\\t\\t\\t-> Recall: {metrics.recall_score(y_test, y_pred,average=\"weighted\")}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Twitter Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_df():    \n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    num_mentions = []\n",
    "    num_url = []\n",
    "    num_hashtags = []\n",
    "    num_emojis = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            num_mentions.append(text.count('@'))\n",
    "            num_url.append(text.count('http://') + text.count('https://'))\n",
    "            num_hashtags.append(text.count('#'))\n",
    "            num_emojis.append(emoji_count(text))\n",
    "\n",
    "    data['mentions'] = num_mentions\n",
    "    data['url'] = num_url\n",
    "    data['hashtags'] = num_hashtags\n",
    "    data['emojis'] = num_emojis\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction\n",
    "\n",
    "For gender prediction, the most relevant twitter features are:\n",
    "- Number of hashtags\n",
    "- Number of emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_twitter_df = twitter_df.copy()\n",
    "outliers = {'hashtags':32,'emojis':100}\n",
    "classifier('RF',gender_twitter_df,['hashtags','emojis'],'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction\n",
    "For age prediction all the twitter features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_twitter_df = twitter_df.copy()\n",
    "outliers = {'mentions':170,'hashtags':32,'emojis':100}\n",
    "classifier('xgboost',age_twitter_df,['mentions','url','hashtags','emojis'],'age',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction\n",
    "For region prediction all the twitter features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_twitter_df = twitter_df.copy()\n",
    "outliers = {'mentions':150,'hashtags':32,'url':75,'emojis':80}\n",
    "classifier('xgboost',region_twitter_df,['mentions','url','hashtags','emojis'],'region',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stylistic Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Character Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_character_based():    \n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    punctuation_list = list(string.punctuation)\n",
    "\n",
    "    num_char = []\n",
    "    num_capital = []\n",
    "    num_punctuation = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            num_char.append(len(text))\n",
    "            num_capital.append(sum(1 for c in text if c.isupper()))\n",
    "            num_punctuation.append(sum(1 for c in text if c in punctuation_list))\n",
    "\n",
    "    data['characters'] = num_char\n",
    "    data['capital_letters'] = num_capital\n",
    "    data['punctuations'] = num_punctuation\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_based_df = df_character_based()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction\n",
    "For gender prediction, the most relevant features are: \n",
    "- Number of capital letters\n",
    "- Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_char_df = char_based_df.copy()\n",
    "outliers = {'characters':24000,'capital_letters':800}\n",
    "classifier('xgboost',gender_char_df,['characters','capital_letters'],'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction\n",
    "For age prediction all features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_char_df = char_based_df.copy()\n",
    "outliers = {'characters':20000,'capital_letters':680,'punctuations':750}\n",
    "classifier('xgboost',age_char_df,['characters','capital_letters','punctuations'],'age',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction\n",
    "For region prediction all features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_char_df = char_based_df.copy()\n",
    "outliers = {'characters':22000,'capital_letters':680,'punctuations':750}\n",
    "classifier('xgboost',region_char_df,['characters','capital_letters','punctuations'],'region',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Structural Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_structural_based():    \n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    num_sentence = []\n",
    "    av_sentence_par = []\n",
    "    av_words_par = []\n",
    "    av_char_par = []\n",
    "    variation = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            num_par = len(text.split('\\n'))\n",
    "\n",
    "            sentences = text.split('.')\n",
    "            num_sentence_user = len(sentences)\n",
    "            num_sentence.append(num_sentence_user)\n",
    "            av_sentence_par.append(num_sentence_user/num_par)\n",
    "            \n",
    "            words = [w for w in text.split(' ') if len(w) > 0]\n",
    "            num_words = len(words)\n",
    "            av_words_par.append(num_words/num_par)\n",
    "            av_char_par.append(len(text)/num_par)\n",
    "\n",
    "            len_sentence_list = [len(sentence) for sentence in sentences]\n",
    "            variation.append(stat.variance(len_sentence_list))     \n",
    "\n",
    "    data['num_sentence'] = num_sentence\n",
    "    data['av_sentence_par'] = av_sentence_par\n",
    "    data['av_words_par'] = av_words_par\n",
    "    data['av_char_par'] = av_char_par\n",
    "    data['variation'] = variation\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_based_df = df_structural_based()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction\n",
    "For gender prediction, the most relevant features are:\n",
    "- Sentence count\n",
    "- Av count of sentence per paragraph\n",
    "- Av count of words per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_struct_df = struct_based_df.copy()\n",
    "outliers = {'num_sentence':260,'av_sentence_par':2.2}\n",
    "classifier('xgboost',gender_struct_df,['num_sentence','av_sentence_par','av_words_par'],'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction\n",
    "For age prediction, all features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_struct_df = struct_based_df.copy()\n",
    "outliers = {'num_sentence':280,'av_sentence_par':2,'av_words_par':20,'av_char_par':130}\n",
    "classifier('xgboost',age_struct_df,['num_sentence','av_sentence_par','av_words_par','av_char_par','variation'],'age',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction\n",
    "For region prediction, all features are relevant except average number of words per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_struct_df = struct_based_df.copy()\n",
    "outliers = {'num_sentence':260,'av_sentence_par':1.8,'av_char_par':130}\n",
    "classifier('xgboost',region_struct_df,['num_sentence','av_sentence_par','av_char_par','variation'],'region',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Syntactic Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ü\",\"u\"),\n",
    "        (\"ñ\",\"n\"),\n",
    "        (\"ç\",\"c\"),\n",
    "        (\"\\u2026\",\"...\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents as tweets have them removed\n",
    "# PoS Tag\n",
    "def tagger():\n",
    "    oraciones = cess_esp.tagged_sents()\n",
    "    oraciones_sin_acentos = []\n",
    "    for oracion in oraciones:\n",
    "        oracion_sin_acento = []\n",
    "        for palabra,tag in oracion:\n",
    "            palabra = normalize(palabra)\n",
    "            oracion_sin_acento.append((palabra,tag))\n",
    "        oraciones_sin_acentos.append(oracion_sin_acento)\n",
    "\n",
    "    return nltk.UnigramTagger(oraciones_sin_acentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_syntactic_based():\n",
    "\n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    pos_tag = tagger()\n",
    "\n",
    "    num_det = []\n",
    "    num_pre = []\n",
    "    num_sing = []\n",
    "    num_plural = []\n",
    "    num_adv = []\n",
    "    num_adj = []\n",
    "    num_prop = []\n",
    "    num_pronouns = []\n",
    "    num_past = []\n",
    "    num_future = []\n",
    "    num_conj = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            # lowercase and remove punctuation marks\n",
    "            text = text.lower()\n",
    "            text = text.translate(str.maketrans('','',string.punctuation))\n",
    "            analysis = pos_tag.tag(text.split(' '))\n",
    "\n",
    "            det = 0\n",
    "            pre = 0\n",
    "            sing = 0\n",
    "            plural = 0\n",
    "            adv = 0\n",
    "            adj = 0\n",
    "            prop = 0\n",
    "            pronouns = 0\n",
    "            past = 0\n",
    "            future = 0\n",
    "            conj = 0\n",
    "\n",
    "            for word,tag in analysis:\n",
    "                if tag != None:\n",
    "                    if tag[0] == 'd':\n",
    "                        det += 1\n",
    "                    elif tag[0] == 'a':\n",
    "                        adj += 1\n",
    "                    elif tag[0] == 'c':\n",
    "                        conj += 1\n",
    "                    elif tag[0] == 'p':\n",
    "                        pronouns += 1\n",
    "                    elif tag[0] == 'n':\n",
    "                        if tag[1] == 'p':\n",
    "                            prop += 1\n",
    "                        if tag[3] == 's':\n",
    "                            sing += 1\n",
    "                        elif tag[3] == 'p':\n",
    "                            plural += 1\n",
    "                    elif tag[0] == 'r':\n",
    "                        adv += 1\n",
    "                    elif (tag[0] == 'v' and tag[3] == 'f'):\n",
    "                        future += 1\n",
    "                    elif (tag[0] == 'v' and tag[3] == 's'):\n",
    "                        past += 1\n",
    "                    elif tag[0] == 's':\n",
    "                        pre += 1\n",
    "\n",
    "            \n",
    "            num_det.append(det)\n",
    "            num_pre.append(pre)\n",
    "            num_sing.append(sing)\n",
    "            num_plural.append(plural)\n",
    "            num_adv.append(adv)\n",
    "            num_adj.append(adj)\n",
    "            num_prop.append(prop)\n",
    "            num_pronouns.append(pronouns)\n",
    "            num_past.append(past)\n",
    "            num_future.append(future)\n",
    "            num_conj.append(conj)\n",
    "\n",
    "    data['num_det'] = num_det\n",
    "    data['num_pre'] = num_pre\n",
    "    data['num_sing'] = num_sing\n",
    "    data['num_plural'] = num_plural\n",
    "    data['num_adv'] = num_adv\n",
    "    data['num_adj'] = num_adj\n",
    "    data['num_prop'] = num_prop\n",
    "    data['num_pronouns'] = num_pronouns\n",
    "    data['num_past'] = num_past\n",
    "    data['num_future'] = num_future\n",
    "    data['num_conj'] = num_conj\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for character based features analysis\n",
    "def df_syntactic_based_ratios():\n",
    "\n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    pos_tag = tagger()\n",
    "\n",
    "    num_det = []\n",
    "    num_pre = []\n",
    "    num_sing = []\n",
    "    num_plural = []\n",
    "    num_adv = []\n",
    "    num_adj = []\n",
    "    num_prop = []\n",
    "    num_pronouns = []\n",
    "    num_past = []\n",
    "    num_future = []\n",
    "    num_conj = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            # lowercase and remove punctuation marks\n",
    "            text = text.lower()\n",
    "            text = text.translate(str.maketrans('','',string.punctuation))\n",
    "            analysis = pos_tag.tag(text.split(' '))\n",
    "\n",
    "            num_words = len(analysis)\n",
    "\n",
    "            det = 0\n",
    "            pre = 0\n",
    "            sing = 0\n",
    "            plural = 0\n",
    "            adv = 0\n",
    "            adj = 0\n",
    "            prop = 0\n",
    "            pronouns = 0\n",
    "            past = 0\n",
    "            future = 0\n",
    "            conj = 0\n",
    "\n",
    "            for word,tag in analysis:\n",
    "                if tag != None:\n",
    "                    if tag[0] == 'd':\n",
    "                        det += 1\n",
    "                    elif tag[0] == 'a':\n",
    "                        adj += 1\n",
    "                    elif tag[0] == 'c':\n",
    "                        conj += 1\n",
    "                    elif tag[0] == 'p':\n",
    "                        pronouns += 1\n",
    "                    elif tag[0] == 'n':\n",
    "                        if tag[1] == 'p':\n",
    "                            prop += 1\n",
    "                        if tag[3] == 's':\n",
    "                            sing += 1\n",
    "                        elif tag[3] == 'p':\n",
    "                            plural += 1\n",
    "                    elif tag[0] == 'r':\n",
    "                        adv += 1\n",
    "                    elif (tag[0] == 'v' and tag[3] == 'f'):\n",
    "                        future += 1\n",
    "                    elif (tag[0] == 'v' and tag[3] == 's'):\n",
    "                        past += 1\n",
    "                    elif tag[0] == 's':\n",
    "                        pre += 1\n",
    "\n",
    "            \n",
    "            num_det.append(det/num_words)\n",
    "            num_pre.append(pre/num_words)\n",
    "            num_sing.append(sing/num_words)\n",
    "            num_plural.append(plural/num_words)\n",
    "            num_adv.append(adv/num_words)\n",
    "            num_adj.append(adj/num_words)\n",
    "            num_prop.append(prop/num_words)\n",
    "            num_pronouns.append(pronouns/num_words)\n",
    "            num_past.append(past/num_words)\n",
    "            num_future.append(future/num_words)\n",
    "            num_conj.append(conj/num_words)\n",
    "\n",
    "    data['ratio_det'] = num_det\n",
    "    data['ratio_pre'] = num_pre\n",
    "    data['ratio_sing'] = num_sing\n",
    "    data['ratio_plural'] = num_plural\n",
    "    data['ratio_adv'] = num_adv\n",
    "    data['ratio_adj'] = num_adj\n",
    "    data['ratio_prop'] = num_prop\n",
    "    data['ratio_pronouns'] = num_pronouns\n",
    "    data['ratio_past'] = num_past\n",
    "    data['ratio_future'] = num_future\n",
    "    data['ratio_conj'] = num_conj\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "synt_based_df = df_syntactic_based()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>region</th>\n",
       "      <th>ratio_det</th>\n",
       "      <th>ratio_pre</th>\n",
       "      <th>ratio_sing</th>\n",
       "      <th>ratio_plural</th>\n",
       "      <th>ratio_adv</th>\n",
       "      <th>ratio_adj</th>\n",
       "      <th>ratio_prop</th>\n",
       "      <th>ratio_pronouns</th>\n",
       "      <th>ratio_past</th>\n",
       "      <th>ratio_future</th>\n",
       "      <th>ratio_conj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lozanogarcia68</td>\n",
       "      <td>female</td>\n",
       "      <td>55+</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>0.122970</td>\n",
       "      <td>0.127610</td>\n",
       "      <td>0.120650</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.060325</td>\n",
       "      <td>0.058005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066125</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.035963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beltrangmodet</td>\n",
       "      <td>male</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edubellver</td>\n",
       "      <td>male</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>0.133858</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luss_27</td>\n",
       "      <td>female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.098605</td>\n",
       "      <td>0.092093</td>\n",
       "      <td>0.026977</td>\n",
       "      <td>0.073488</td>\n",
       "      <td>0.031628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109767</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.044651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k15ce</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>0.158570</td>\n",
       "      <td>0.119157</td>\n",
       "      <td>0.121907</td>\n",
       "      <td>0.027498</td>\n",
       "      <td>0.055912</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082493</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.049496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username  gender    age  region  ratio_det  ratio_pre  ratio_sing  \\\n",
       "0  lozanogarcia68  female    55+  Madrid   0.122970   0.127610    0.120650   \n",
       "1   beltrangmodet    male  18-24  Madrid   0.095745   0.170213    0.021277   \n",
       "2      edubellver    male  18-24  Madrid   0.133858   0.141732    0.094488   \n",
       "3         luss_27  female  18-24  Madrid   0.134884   0.098605    0.092093   \n",
       "4           k15ce    male  25-34  Madrid   0.158570   0.119157    0.121907   \n",
       "\n",
       "   ratio_plural  ratio_adv  ratio_adj  ratio_prop  ratio_pronouns  ratio_past  \\\n",
       "0      0.032483   0.060325   0.058005         0.0        0.066125    0.003480   \n",
       "1      0.021277   0.063830   0.021277         0.0        0.117021    0.000000   \n",
       "2      0.031496   0.015748   0.039370         0.0        0.078740    0.000000   \n",
       "3      0.026977   0.073488   0.031628         0.0        0.109767    0.002791   \n",
       "4      0.027498   0.055912   0.042163         0.0        0.082493    0.010999   \n",
       "\n",
       "   ratio_future  ratio_conj  \n",
       "0      0.002320    0.035963  \n",
       "1      0.000000    0.031915  \n",
       "2      0.000000    0.031496  \n",
       "3      0.001860    0.044651  \n",
       "4      0.000917    0.049496  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synt_based_ratio_df = df_syntactic_based_ratios()\n",
    "synt_based_ratio_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction\n",
    "For gender prediction, the most relevant features are:\n",
    "- Num of determiners\n",
    "- Num of prepositions\n",
    "- Num of singular nouns\n",
    "- Num of adjetives\n",
    "- Num of future tense verbs\n",
    "- Num of conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_synt_df = synt_based_df.copy()\n",
    "outliers = {'num_det':450,'num_pre':450,'num_sing':400,'num_adj':150,'num_future':8,'num_conj':150}\n",
    "classifier('xgboost',gender_synt_df,['num_det','num_pre','num_sing','num_adj','num_future','num_conj'],'gender',outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_synt_df = synt_based_ratio_df.copy()\n",
    "outliers = None\n",
    "classifier('xgboost',gender_synt_df,['ratio_det','ratio_pre','ratio_sing','ratio_adj','ratio_pronouns','ratio_future'],'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction\n",
    "For age prediction, the most relevant features are:\n",
    "- Num of plural nouns\n",
    "- Num of adverbs\n",
    "- Num of singular nouns\n",
    "- Num of past tense verbs\n",
    "- Num of future tense verbs\n",
    "- Num of prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_synt_df = synt_based_df.copy()\n",
    "outliers = {'num_plural':90,'num_adv':120,'num_sing':380,'num_past':17,'num_future':8,'num_pre':450}\n",
    "classifier('xgboost',age_synt_df,['num_plural','num_adv','num_sing','num_past','num_future','num_pre'],'age',outliers,type='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_synt_df = synt_based_ratio_df.copy()\n",
    "outliers = None\n",
    "classifier('xgboost',age_synt_df,['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future'],'age',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction\n",
    "For region prediction, the most relevant features are:\n",
    "- Num of determiners\n",
    "- Num of prepositions\n",
    "- Num of adverbs\n",
    "- Num of past tense verbs\n",
    "- Num of future tense verbs\n",
    "- Num of conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_synt_df = synt_based_df.copy()\n",
    "outliers = {'num_det':450,'num_pre':470,'num_past':18,'num_future':8,'num_conj':140}\n",
    "classifier('xgboost',region_synt_df,['num_det','num_pre','num_adv','num_past','num_future','num_conj'],'region',outliers,type='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_synt_df = synt_based_ratio_df.copy()\n",
    "outliers = None\n",
    "classifier('xgboost',region_synt_df,['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future'],'region',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Word Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_analysis_data = pd.read_csv('../Spanish-NRC-EmoLex.txt',sep='\\t')\n",
    "spanish_dict = [normalize(w) for w in list(sent_analysis_data['Spanish Word'])]\n",
    "columns = sent_analysis_data.keys()\n",
    "negative_cols = ['negative','fear','anger','disgust','sadness']\n",
    "positive_cols = ['positive','joy','trust']\n",
    "\n",
    "negative_words = []\n",
    "for col in negative_cols:\n",
    "    i = 0\n",
    "    for val in sent_analysis_data[col]:\n",
    "        if val == 1:\n",
    "            negative_words.append(spanish_dict[i])\n",
    "        i += 1\n",
    "\n",
    "positive_words = []\n",
    "for col in positive_cols:\n",
    "    i = 0\n",
    "    for val in sent_analysis_data[col]:\n",
    "        if val == 1:\n",
    "            positive_words.append(spanish_dict[i])\n",
    "        i += 1\n",
    "\n",
    "negative_words = set(negative_words)\n",
    "positive_words = set(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_analysis(words):\n",
    "\n",
    "    intersection_neg = list(negative_words & set(words))\n",
    "    intersection_pos = list(positive_words & set(words))\n",
    "\n",
    "    return len(intersection_pos),len(intersection_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of words that occur exactly 'num' times or None if no coincidence\n",
    "def num_occurences(lista,num):\n",
    "    aux_dict = {}\n",
    "    for item in lista:\n",
    "        if item in aux_dict.keys():\n",
    "            aux_dict[item] += 1\n",
    "        else:\n",
    "            aux_dict[item] = 1\n",
    "\n",
    "    try:\n",
    "        idx = list(aux_dict.values()).index(num)\n",
    "        words = list(aux_dict.keys())[idx]\n",
    "        return len(words)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for character based features analysis\n",
    "def df_word_based():\n",
    "\n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    # load spanish stop words\n",
    "    stop_words_df = pd.read_csv('../spanish-stop-words.txt',header=None)\n",
    "    stop_words = [normalize(w) for w in list(stop_words_df[0])]\n",
    "\n",
    "    num_words = []\n",
    "    num_pos_words = []\n",
    "    num_neg_words = []\n",
    "    num_unique = []\n",
    "    num_twice = []\n",
    "    av_length = []\n",
    "    max_length = []\n",
    "    num_numbers = []\n",
    "    num_greater = []\n",
    "    num_smaller = []\n",
    "    num_stop = []\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            text = f.read()\n",
    "            # remove punctuation\n",
    "            text = text.translate(str.maketrans('','',string.punctuation))\n",
    "            sentences = text.split('\\n')\n",
    "            words = []\n",
    "            for sentence in sentences:\n",
    "                words += sentence.split(' ')\n",
    "            words = [w for w in words if len(w) != 0]\n",
    "            words_lower = [w.lower() for w in words if len(w) != 0]\n",
    "\n",
    "            num_words.append(len(words))\n",
    "\n",
    "            pos, neg = sent_analysis(words_lower)\n",
    "\n",
    "            num_pos_words.append(pos)\n",
    "            num_neg_words.append(neg)\n",
    "\n",
    "            # unique words\n",
    "            num_unique.append(num_occurences(words,1))\n",
    "            # twice occurrences\n",
    "            num_twice.append(num_occurences(words,2))\n",
    "            \n",
    "            # max, av, >6, <3 length and num words with digits, count english words\n",
    "            max_len = 0\n",
    "            sum_length = 0\n",
    "            digits = 0\n",
    "            len_greater = 0\n",
    "            len_smaller = 0\n",
    "            for word in words:\n",
    "                sum_length += len(word)\n",
    "                if len(word) > max_len:\n",
    "                    max_len = len(word)\n",
    "\n",
    "                if len(re.findall('\\d',word)) > 0:\n",
    "                    digits += 1\n",
    "\n",
    "                if len(word) > 6:\n",
    "                    len_greater += 1\n",
    "                elif len(word) < 3:\n",
    "                    len_smaller += 1\n",
    "            \n",
    "            av_length.append(sum_length/len(words))\n",
    "            max_length.append(max_len)\n",
    "            num_numbers.append(digits)\n",
    "            num_greater.append(len_greater)\n",
    "            num_smaller.append(len_smaller)\n",
    "\n",
    "            # count stop-words \n",
    "            intersection_stop = list(set(stop_words) & set(words_lower))\n",
    "            num_stop.append(len(intersection_stop))\n",
    "\n",
    "\n",
    "    data['num_words'] = num_words\n",
    "    data['num_pos_words'] = num_pos_words\n",
    "    data['num_neg_words'] = num_neg_words\n",
    "    data['num_unique'] = num_unique\n",
    "    data['num_twice'] = num_twice\n",
    "    data['av_length'] = av_length\n",
    "    data['max_length'] = max_length\n",
    "    data['num_numbers'] = num_numbers\n",
    "    data['num_greater'] = num_greater\n",
    "    data['num_smaller'] = num_smaller\n",
    "    data['num_stop'] = num_stop\n",
    "\n",
    "    return data         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_based_df = df_word_based()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction\n",
    "For gender prediction, the most relevant features are:\n",
    "- Num of words\n",
    "- Num of negative words\n",
    "- Average word length\n",
    "- Max word length\n",
    "- Num of words with numbers\n",
    "- Num of words of length smaller than 3\n",
    "- Num of stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_word_df = word_based_df.copy()\n",
    "outliers = {'num_words':3200,'num_neg_words':110,'av_length':6,'max_length':52,'num_numbers':50,'num_smaller':900}\n",
    "classifier('xgboost',gender_word_df,['num_words','num_neg_words','av_length','max_length','num_numbers','num_smaller','num_stop'],'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction\n",
    "For age prediction, the most relevant features are all except 'num of unique words' and 'number of words that occur twice'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_word_df = word_based_df.copy()\n",
    "outliers = {'num_words':3500,'num_pos_words':120,'num_neg_words':110,'av_length':6,'max_length':52,'num_numbers':50,'num_greater':900,'num_smaller':900}\n",
    "classifier('xgboost',age_word_df,['num_words','num_pos_words','num_neg_words','av_length','max_length','num_numbers','num_greater','num_smaller','num_stop'],'age',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction\n",
    "For region prediction, the most relevant features are all except 'number of positive words' and 'number of words that occur twice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_word_df = word_based_df.copy()\n",
    "outliers = {'num_words':3500,'num_neg_words':110,'num_unique':10,'av_length':6,'max_length':50,'num_numbers':50,'num_greater':900,'num_smaller':900}\n",
    "classifier('xgboost',region_word_df,['num_words','num_neg_words','num_unique','av_length','max_length','num_numbers','num_greater','num_smaller','num_stop'],'region',outliers,type='multi')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 All stylistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = char_based_df.copy()\n",
    "struct_df = struct_based_df.copy()\n",
    "synt_df = synt_based_ratio_df.copy()\n",
    "word_df = word_based_df.copy()\n",
    "\n",
    "stylistic_df = pd.concat([char_df,struct_df,synt_df,word_df],axis=1)\n",
    "stylistic_df = stylistic_df.iloc[:,~stylistic_df.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>region</th>\n",
       "      <th>characters</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>av_sentence_par</th>\n",
       "      <th>av_words_par</th>\n",
       "      <th>...</th>\n",
       "      <th>num_pos_words</th>\n",
       "      <th>num_neg_words</th>\n",
       "      <th>num_unique</th>\n",
       "      <th>num_twice</th>\n",
       "      <th>av_length</th>\n",
       "      <th>max_length</th>\n",
       "      <th>num_numbers</th>\n",
       "      <th>num_greater</th>\n",
       "      <th>num_smaller</th>\n",
       "      <th>num_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lozanogarcia68</td>\n",
       "      <td>female</td>\n",
       "      <td>55+</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>5731</td>\n",
       "      <td>233</td>\n",
       "      <td>390</td>\n",
       "      <td>208</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>17.220000</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5.104839</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>237</td>\n",
       "      <td>241</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beltrangmodet</td>\n",
       "      <td>male</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>554</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4.316832</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edubellver</td>\n",
       "      <td>male</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>869</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.521739</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.557047</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luss_27</td>\n",
       "      <td>female</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>7044</td>\n",
       "      <td>362</td>\n",
       "      <td>437</td>\n",
       "      <td>175</td>\n",
       "      <td>1.535088</td>\n",
       "      <td>9.429825</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4.766931</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>332</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k15ce</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>7117</td>\n",
       "      <td>299</td>\n",
       "      <td>333</td>\n",
       "      <td>224</td>\n",
       "      <td>2.055046</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.737913</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>263</td>\n",
       "      <td>366</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         username  gender    age  region  characters  capital_letters  \\\n",
       "0  lozanogarcia68  female    55+  Madrid        5731              233   \n",
       "1   beltrangmodet    male  18-24  Madrid         554               38   \n",
       "2      edubellver    male  18-24  Madrid         869               17   \n",
       "3         luss_27  female  18-24  Madrid        7044              362   \n",
       "4           k15ce    male  25-34  Madrid        7117              299   \n",
       "\n",
       "   punctuations  num_sentence  av_sentence_par  av_words_par  ...  \\\n",
       "0           390           208         4.160000     17.220000  ...   \n",
       "1            16            12         1.200000      9.400000  ...   \n",
       "2            42            23         1.000000      5.521739  ...   \n",
       "3           437           175         1.535088      9.429825  ...   \n",
       "4           333           224         2.055046     10.000000  ...   \n",
       "\n",
       "   num_pos_words  num_neg_words  num_unique  num_twice  av_length  max_length  \\\n",
       "0             44             31           6          4   5.104839          21   \n",
       "1              6              1           6          4   4.316832          15   \n",
       "2             11             10           2          4   4.557047          14   \n",
       "3             41             33           5          3   4.766931          44   \n",
       "4             50             34           4          5   4.737913          45   \n",
       "\n",
       "   num_numbers  num_greater  num_smaller  num_stop  \n",
       "0           13          237          241       131  \n",
       "1            1           17           24        32  \n",
       "2            0           29           28        46  \n",
       "3            2          228          332       148  \n",
       "4            8          263          366       151  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_df = stylistic_df.copy()\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df = stylistic_df.copy()\n",
    "\n",
    "outliers = {'characters':24000,\n",
    "            'capital_letters':800,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':2.2,\n",
    "            'num_words':3200,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_adj','ratio_pronouns','ratio_future']\n",
    "\n",
    "classifier('xgboost',gender_df,features,'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = stylistic_df.copy()\n",
    "\n",
    "outliers = {'characters':20000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':280,\n",
    "            'av_sentence_par':2,\n",
    "            'av_words_par':20,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_pos_words':120,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',age_df,features,'age',outliers,type='mult')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = stylistic_df.copy()\n",
    "\n",
    "outliers = {'characters':22000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':1.8,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_neg_words':110,\n",
    "            'num_unique':10,\n",
    "            'av_length':6,\n",
    "            'max_length':50,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',region_df,features,'region',outliers,type='mult')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spanish stop words and remove accents (tweets dont have accents)\n",
    "stop_words_df = pd.read_csv('../spanish-stop-words.txt',header=None)\n",
    "stop_words = [normalize(w) for w in list(stop_words_df[0])] + ['q','ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_df():    \n",
    "    # create df\n",
    "    data = pd.read_excel('cleaned_users.xlsx')\n",
    "    username_list = data['username']\n",
    "\n",
    "    texts = []\n",
    "    wordgram = []\n",
    "    chargram = []\n",
    "    char_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,7))\n",
    "    word_vectorizer = TfidfVectorizer(stop_words=stop_words,ngram_range=(1,2))\n",
    "\n",
    "    for username in username_list:\n",
    "        with open(f'Cleaned Documents/{username}.txt','r') as f:\n",
    "            document = f.read()\n",
    "            texts.append(document)\n",
    "\n",
    "            tweets = document.split('\\n')\n",
    "            # only consider users with 90 or more tweets\n",
    "            if len(tweets) >= 90:\n",
    "                word_tfidf_matrix = word_vectorizer.fit_transform(tweets)\n",
    "                char_tfidf_matrix = char_vectorizer.fit_transform(tweets)\n",
    "                # normalize matrix to perform PCA\n",
    "                matrix_word = word_tfidf_matrix.todense()\n",
    "                word_matrix_df = pd.DataFrame(matrix_word)\n",
    "                word_matrix_df = word_matrix_df - word_matrix_df.mean()\n",
    "\n",
    "                matrix_char = char_tfidf_matrix.todense()\n",
    "                char_matrix_df = pd.DataFrame(matrix_char)\n",
    "                char_matrix_df = char_matrix_df - char_matrix_df.mean()\n",
    "                # perform PCA to reduce dimensionality (vector of dim=85 for each tweet)\n",
    "                pca = PCA(n_components=90)\n",
    "\n",
    "                final_word_matrix = pd.DataFrame(pca.fit_transform(word_matrix_df))\n",
    "                final_char_matrix = pd.DataFrame(pca.fit_transform(char_matrix_df))\n",
    "                # obtain author's vector by averaging the vectors of their tweets\n",
    "                author_word_vector = np.array(final_word_matrix.mean(axis=0))\n",
    "                author_char_vector = np.array(final_char_matrix.mean(axis=0))\n",
    "                \n",
    "                wordgram.append(author_word_vector)\n",
    "                chargram.append(author_char_vector)\n",
    "\n",
    "            else:\n",
    "                wordgram.append(None)\n",
    "                chargram.append(None)\n",
    "\n",
    "    data['text'] = texts\n",
    "    data['wordgram'] = wordgram\n",
    "    data['chargram'] = chargram\n",
    "\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_df = ngram_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Word Gram "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_wordgram_df = ngram_df.copy()\n",
    "gender_wordgram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',gender_wordgram_df,['wordgram'],'gender',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_wordgram_df = ngram_df.copy()\n",
    "age_wordgram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',age_wordgram_df,['wordgram'],'age',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_wordgram_df = ngram_df.copy()\n",
    "region_wordgram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',region_wordgram_df,['wordgram'],'region',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Character Gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_chargram_df = ngram_df.copy()\n",
    "gender_chargram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',gender_chargram_df,['chargram'],'gender',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_chargram_df = ngram_df.copy()\n",
    "age_chargram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',age_chargram_df,['chargram'],'age',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_chargram_df = ngram_df.copy()\n",
    "region_chargram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',region_chargram_df,['chargram'],'region',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Both"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_ngram_df = ngram_df.copy()\n",
    "gender_ngram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',gender_ngram_df,['wordgram','chargram'],'gender',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ngram_df = ngram_df.copy()\n",
    "age_ngram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',age_ngram_df,['wordgram','chargram'],'age',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ngram_df = ngram_df.copy()\n",
    "region_ngram_df.dropna(inplace=True)\n",
    "\n",
    "classifier('xgboost',region_ngram_df,['wordgram','chargram'],'region',type='multi',normalize=False,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TF + SF (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = stylistic_df.copy()\n",
    "tf_df = twitter_df.copy()\n",
    "\n",
    "combi1_df = pd.concat([sf_df,tf_df],axis=1)\n",
    "combi1_df = combi1_df.iloc[:,~combi1_df.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_combi1_df = combi1_df.copy()\n",
    "\n",
    "outliers = {'hashtags':32,\n",
    "            'emojis':100,\n",
    "            'characters':24000,\n",
    "            'capital_letters':800,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':2.2,\n",
    "            'num_words':3200,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_adj','ratio_pronouns','ratio_future']\n",
    "\n",
    "classifier('xgboost',gender_combi1_df,features,'gender',outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_combi1_df = combi1_df.copy()\n",
    "\n",
    "outliers = {'mentions':170,\n",
    "            'hashtags':32,\n",
    "            'emojis':100,\n",
    "            'characters':20000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':280,\n",
    "            'av_sentence_par':2,\n",
    "            'av_words_par':20,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_pos_words':120,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',age_combi1_df,features,'age',outliers,type='mult')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "region_combi1_df = combi1_df.copy()\n",
    "\n",
    "outliers = {'mentions':150,\n",
    "            'hashtags':32,\n",
    "            'url':75,\n",
    "            'emojis':80,\n",
    "            'characters':22000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':1.8,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_neg_words':110,\n",
    "            'num_unique':10,\n",
    "            'av_length':6,\n",
    "            'max_length':50,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "classifier('xgboost',region_combi1_df,features,'region',outliers,type='mult')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TF + N-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_df = ngram_df.copy()\n",
    "tf_df = twitter_df.copy()\n",
    "\n",
    "combi2_df = pd.concat([ng_df,tf_df],axis=1)\n",
    "combi2_df = combi2_df.iloc[:,~combi2_df.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_combi2_df = combi2_df.copy()\n",
    "gender_combi2_df.dropna(inplace=True)\n",
    "gender_combi2_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'hashtags':32,\n",
    "            'emojis':100\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] \n",
    "\n",
    "classifier('xgboost',gender_combi2_df,features,'gender',outliers,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_combi2_df = combi2_df.copy()\n",
    "age_combi2_df.dropna(inplace=True)\n",
    "age_combi2_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'mentions':170,\n",
    "            'hashtags':32,\n",
    "            'emojis':100\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] \n",
    "\n",
    "classifier('xgboost',age_combi2_df,features,'age',outliers,type='multi',ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_combi2_df = combi2_df.copy()\n",
    "region_combi2_df.dropna(inplace=True)\n",
    "region_combi2_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'mentions':150,\n",
    "            'hashtags':32,\n",
    "            'url':75,\n",
    "            'emojis':80\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] \n",
    "\n",
    "classifier('xgboost',region_combi2_df,features,'region',outliers,type='multi',ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SF + N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_df = ngram_df.copy()\n",
    "sf_df = stylistic_df.copy()\n",
    "\n",
    "combi3_df = pd.concat([ng_df,sf_df],axis=1)\n",
    "combi3_df = combi3_df.iloc[:,~combi3_df.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_combi3_df = combi3_df.copy()\n",
    "gender_combi3_df.dropna(inplace=True)\n",
    "gender_combi3_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'characters':24000,\n",
    "            'capital_letters':800,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':2.2,\n",
    "            'num_words':3200,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_adj','ratio_pronouns','ratio_future']\n",
    "\n",
    "classifier('xgboost',gender_combi3_df,features,'gender',outliers,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_combi3_df = combi3_df.copy()\n",
    "age_combi3_df.dropna(inplace=True)\n",
    "age_combi3_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'characters':20000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':280,\n",
    "            'av_sentence_par':2,\n",
    "            'av_words_par':20,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_pos_words':120,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',age_combi3_df,features,'age',outliers,type='mult',ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "region_combi3_df = combi3_df.copy()\n",
    "region_combi3_df.dropna(inplace=True)\n",
    "region_combi3_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'characters':22000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':1.8,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_neg_words':110,\n",
    "            'num_unique':10,\n",
    "            'av_length':6,\n",
    "            'max_length':50,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',region_combi3_df,features,'region',outliers,type='mult',ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. TF + SF + N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_df = ngram_df.copy()\n",
    "sf_df = stylistic_df.copy()\n",
    "tf_df = twitter_df.copy()\n",
    "\n",
    "combi4_df = pd.concat([ng_df,sf_df,tf_df],axis=1)\n",
    "combi4_df = combi4_df.iloc[:,~combi4_df.columns.duplicated()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_combi4_df = combi4_df.copy()\n",
    "gender_combi4_df.dropna(inplace=True)\n",
    "gender_combi4_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'hashtags':32,\n",
    "            'emojis':100,\n",
    "            'characters':24000,\n",
    "            'capital_letters':800,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':2.2,\n",
    "            'num_words':3200,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_adj','ratio_pronouns','ratio_future']\n",
    "\n",
    "classifier('xgboost',gender_combi4_df,features,'gender',outliers,ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_combi4_df = combi4_df.copy()\n",
    "age_combi4_df.dropna(inplace=True)\n",
    "age_combi4_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'mentions':170,\n",
    "            'hashtags':32,\n",
    "            'emojis':100,\n",
    "            'characters':20000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':280,\n",
    "            'av_sentence_par':2,\n",
    "            'av_words_par':20,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_pos_words':120,\n",
    "            'num_neg_words':110,\n",
    "            'av_length':6,\n",
    "            'max_length':52,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',age_combi4_df,features,'age',outliers,type='mult',ngram=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "region_combi4_df = combi4_df.copy()\n",
    "region_combi4_df.dropna(inplace=True)\n",
    "region_combi4_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "outliers = {'mentions':150,\n",
    "            'hashtags':32,\n",
    "            'url':75,\n",
    "            'emojis':80,\n",
    "            'characters':22000,\n",
    "            'capital_letters':680,\n",
    "            'punctuations':750,\n",
    "            'num_sentence':260,\n",
    "            'av_sentence_par':1.8,\n",
    "            'av_char_par':130,\n",
    "            'num_words':3500,\n",
    "            'num_neg_words':110,\n",
    "            'num_unique':10,\n",
    "            'av_length':6,\n",
    "            'max_length':50,\n",
    "            'num_numbers':50,\n",
    "            'num_greater':900,\n",
    "            'num_smaller':900\n",
    "            }\n",
    "\n",
    "features = list(outliers.keys()) + ['wordgram'] + ['ratio_det','ratio_pre','ratio_sing','ratio_plural','ratio_adv','ratio_adj','ratio_pronouns','ratio_past','ratio_future']\n",
    "\n",
    "classifier('xgboost',region_combi4_df,features,'region',outliers,type='multi',ngram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d52cdd818097014d9847b1781256baff15e21759efb83f1252a80c5a45f154c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
