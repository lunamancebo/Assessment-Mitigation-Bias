{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from emoji import emoji_count\n",
    "import statistics as stat\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "#URLS_RE = re.compile(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b')\n",
    "URLS_RE = re.compile(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*')\n",
    "\n",
    "LISTING_RE = re.compile(r'^(|[a-z]?|[0-9]{0,3})(\\-|\\.)+( |\\n)')\n",
    "\n",
    "def remove_urls(text):\n",
    "    return URLS_RE.sub('', text)\n",
    "\n",
    "def replace_multi_whitespaces(line):\n",
    "    return ' '.join(line.split())\n",
    "\n",
    "def remove_listing(line):\n",
    "    return LISTING_RE.sub('', line)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.replace('!','')\n",
    "    text = text.replace('\"','')\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ü\",\"u\"),\n",
    "        (\"ñ\",\"n\"),\n",
    "        (\"ç\",\"c\"),\n",
    "        (\"\\u2026\",\"...\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "\n",
    "def remove_stopwords(text,stop_words):\n",
    "    words = text.split(' ')\n",
    "    not_stop_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            not_stop_words.append(word)\n",
    "    return ' '.join(not_stop_words)\n",
    "      \n",
    "\n",
    "def clean_text(text,stop_words):\t\n",
    "\n",
    "    low = text.lower()\n",
    "    norm = normalize(low)\n",
    "    rem_u = remove_urls(norm)\n",
    "    rem_l = remove_listing(rem_u)\n",
    "    rem_w = replace_multi_whitespaces(rem_l)\n",
    "    rem_p = remove_punctuation(rem_w)\n",
    "    rem_s = remove_stopwords(rem_p,stop_words)\n",
    "    text_enc = rem_s.encode('ascii', 'ignore')\n",
    "\n",
    "    return text_enc.decode()\n",
    "\n",
    "def tagger():\n",
    "    oraciones = cess_esp.tagged_sents()\n",
    "    oraciones_sin_acentos = []\n",
    "    for oracion in oraciones:\n",
    "        oracion_sin_acento = []\n",
    "        for palabra,tag in oracion:\n",
    "            palabra = normalize(palabra)\n",
    "            oracion_sin_acento.append((palabra,tag))\n",
    "        oraciones_sin_acentos.append(oracion_sin_acento)\n",
    "\n",
    "    return nltk.UnigramTagger(oraciones_sin_acentos)\n",
    "\n",
    "\n",
    "def load_embedding(file_path):\n",
    "    embedding_vectors = {}\n",
    "    with open(f'embeddings/{file_path}','r') as f:\n",
    "        first_line = f.readline().split(' ')\n",
    "        for line in f.readlines()[1:]:\n",
    "            row = line.split(' ')\n",
    "            # remove accents\n",
    "            word = normalize(row[0])\n",
    "            if word not in embedding_vectors.keys():\n",
    "                embedding_vectors[word] = [float(val) for val in row[1:]]\n",
    "        \n",
    "    return embedding_vectors\n",
    "\n",
    "def load_sentiment_analysis():\n",
    "    sent_analysis_data = pd.read_csv('../Spanish-NRC-EmoLex.txt',sep='\\t')\n",
    "    spanish_dict = [normalize(w) for w in list(sent_analysis_data['Spanish Word'])]\n",
    "    negative_cols = ['negative','fear','anger','disgust','sadness']\n",
    "    positive_cols = ['positive','joy','trust']\n",
    "\n",
    "    negative_words = []\n",
    "    for col in negative_cols:\n",
    "        i = 0\n",
    "        for val in sent_analysis_data[col]:\n",
    "            if val == 1:\n",
    "                negative_words.append(spanish_dict[i])\n",
    "            i += 1\n",
    "\n",
    "    positive_words = []\n",
    "    for col in positive_cols:\n",
    "        i = 0\n",
    "        for val in sent_analysis_data[col]:\n",
    "            if val == 1:\n",
    "                positive_words.append(spanish_dict[i])\n",
    "            i += 1\n",
    "\n",
    "    return set(positive_words),set(negative_words)\n",
    "\n",
    "def sent_analysis(words,positive_words,negative_words):\n",
    "\n",
    "    intersection_neg = list(negative_words & set(words))\n",
    "    intersection_pos = list(positive_words & set(words))\n",
    "\n",
    "    return len(intersection_pos),len(intersection_neg)\n",
    "\n",
    "# returns a list of words that occur exactly 'num' times or None if no coincidence\n",
    "def num_occurences(lista,num):\n",
    "    aux_dict = {}\n",
    "    for item in lista:\n",
    "        if item in aux_dict.keys():\n",
    "            aux_dict[item] += 1\n",
    "        else:\n",
    "            aux_dict[item] = 1\n",
    "\n",
    "    try:\n",
    "        idx = list(aux_dict.values()).index(num)\n",
    "        words = list(aux_dict.keys())[idx]\n",
    "        return len(words)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def save_results(y_test,y_pred,average,tunning=False):\n",
    "\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    prec = precision_score(y_test,y_pred,average=average)\n",
    "    rec = recall_score(y_test,y_pred,average=average)\n",
    "    f1 = f1_score(y_test,y_pred,average=average)\n",
    "\n",
    "    if tunning:\n",
    "        sent = '\\t* With tunning:\\n'\n",
    "    else:\n",
    "        sent = '\\t* Without tunning:\\n'\n",
    "\n",
    "    with open('results.txt','a') as f:\n",
    "        f.write(sent)\n",
    "        f.write(f'\\t\\t-> Accuracy: {acc}\\n')\n",
    "        f.write(f'\\t\\t-> Precision: {prec}\\n')\n",
    "        f.write(f'\\t\\t-> Recall: {rec}\\n')\n",
    "        f.write(f'\\t\\t-> F1-score: {f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding\n",
      "embedding loaded\n"
     ]
    }
   ],
   "source": [
    "# load spanish stop words and remove accents (tweets dont have accents)\n",
    "stop_words_df = pd.read_csv('../spanish-stop-words.txt',header=None)\n",
    "stop_words = [normalize(w) for w in list(stop_words_df[0])] + ['q','ma']\n",
    "\n",
    "data = pd.read_excel('../cleaned_users.xlsx')\n",
    "username_list = data['username']\n",
    "\n",
    "print('loading embedding')\n",
    "embedding_vectors = load_embedding('glove-sbwc.i25.vec')\n",
    "print('embedding loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sentiment analysis\n",
      "sentiment analysis loaded\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[i for i in range(300)])\n",
    "print('loading sentiment analysis')\n",
    "positive_words,negative_words = load_sentiment_analysis()\n",
    "print('sentiment analysis loaded')\n",
    "\n",
    "punctuation_list = list(string.punctuation)\n",
    "pos_tag = tagger()\n",
    "\n",
    "num_mentions = []\n",
    "num_url = []\n",
    "num_hashtags = []\n",
    "num_emojis = []\n",
    "num_char = []\n",
    "num_capital = []\n",
    "num_punctuation = []\n",
    "num_sentence = []\n",
    "av_sentence_par = []\n",
    "av_words_par = []\n",
    "av_char_par = []\n",
    "variation = []\n",
    "num_det = []\n",
    "num_pre = []\n",
    "num_sing = []\n",
    "num_plural = []\n",
    "num_adv = []\n",
    "num_adj = []\n",
    "num_prop = []\n",
    "num_pronouns = []\n",
    "num_past = []\n",
    "num_future = []\n",
    "num_conj = []\n",
    "num_words = []\n",
    "num_pos_words = []\n",
    "num_neg_words = []\n",
    "num_unique = []\n",
    "num_twice = []\n",
    "av_length = []\n",
    "max_length = []\n",
    "num_numbers = []\n",
    "num_greater = []\n",
    "num_smaller = []\n",
    "num_stop = []\n",
    "for username in username_list[0:1]:\n",
    "    list_vector_tweet = []\n",
    "\n",
    "    with open(f'../Documents/{username}.txt','r') as f:\n",
    "        # twitter features\n",
    "        doc = f.read()\n",
    "        num_mentions.append(doc.count('@'))\n",
    "        num_url.append(doc.count('http://') + doc.count('https://'))\n",
    "        num_hashtags.append(doc.count('#'))\n",
    "        num_emojis.append(emoji_count(doc))\n",
    "\n",
    "    with open(f'../Cleaned Documents/{username}.txt','r') as cf:\n",
    "        doc = cf.read()\n",
    "        # character based\n",
    "        num_char.append(len(doc))\n",
    "        num_capital.append(sum(1 for c in doc if c.isupper()))\n",
    "        num_punctuation.append(sum(1 for c in doc if c in punctuation_list))\n",
    "\n",
    "        # structural based\n",
    "        doc = doc.lower()\n",
    "        tweets = doc.split('\\n')\n",
    "        num_par = len(tweets)\n",
    "\n",
    "        sentences = doc.split('.')\n",
    "        num_sentence_user = len(sentences)\n",
    "        num_sentence.append(num_sentence_user)\n",
    "        av_sentence_par.append(num_sentence_user/num_par)\n",
    "        \n",
    "        words = [w for w in doc.split(' ') if len(w) > 0]\n",
    "        av_words_par.append(len(words)/num_par)\n",
    "        av_char_par.append(len(doc)/num_par)\n",
    "\n",
    "        len_sentence_list = [len(sentence) for sentence in sentences]\n",
    "        variation.append(stat.variance(len_sentence_list))   \n",
    "\n",
    "        # syntactical based\n",
    "        # lowercase and remove punctuation marks\n",
    "        doc = doc.translate(str.maketrans('','',string.punctuation))\n",
    "        analysis = pos_tag.tag(doc.split(' '))\n",
    "\n",
    "        det = 0\n",
    "        pre = 0\n",
    "        sing = 0\n",
    "        plural = 0\n",
    "        adv = 0\n",
    "        adj = 0\n",
    "        prop = 0\n",
    "        pronouns = 0\n",
    "        past = 0\n",
    "        future = 0\n",
    "        conj = 0\n",
    "\n",
    "        for word,tag in analysis:\n",
    "            if tag != None:\n",
    "                if tag[0] == 'd':\n",
    "                    det += 1\n",
    "                elif tag[0] == 'a':\n",
    "                    adj += 1\n",
    "                elif tag[0] == 'c':\n",
    "                    conj += 1\n",
    "                elif tag[0] == 'p':\n",
    "                    pronouns += 1\n",
    "                elif tag[0] == 'n':\n",
    "                    if tag[1] == 'p':\n",
    "                        prop += 1\n",
    "                    if tag[3] == 's':\n",
    "                        sing += 1\n",
    "                    elif tag[3] == 'p':\n",
    "                        plural += 1\n",
    "                elif tag[0] == 'r':\n",
    "                    adv += 1\n",
    "                elif (tag[0] == 'v' and tag[3] == 'f'):\n",
    "                    future += 1\n",
    "                elif (tag[0] == 'v' and tag[3] == 's'):\n",
    "                    past += 1\n",
    "                elif tag[0] == 's':\n",
    "                    pre += 1\n",
    "\n",
    "        \n",
    "        num_det.append(det)\n",
    "        num_pre.append(pre)\n",
    "        num_sing.append(sing)\n",
    "        num_plural.append(plural)\n",
    "        num_adv.append(adv)\n",
    "        num_adj.append(adj)\n",
    "        num_prop.append(prop)\n",
    "        num_pronouns.append(pronouns)\n",
    "        num_past.append(past)\n",
    "        num_future.append(future)\n",
    "        num_conj.append(conj)\n",
    "\n",
    "        # word based\n",
    "        num_words.append(len(words))\n",
    "\n",
    "        pos, neg = sent_analysis(words,positive_words,negative_words)\n",
    "\n",
    "        num_pos_words.append(pos)\n",
    "        num_neg_words.append(neg)\n",
    "\n",
    "        # unique words\n",
    "        num_unique.append(num_occurences(words,1))\n",
    "        # twice occurrences\n",
    "        num_twice.append(num_occurences(words,2))\n",
    "        \n",
    "        # max, av, >6, <3 length and num words with digits, count english words\n",
    "        max_len = 0\n",
    "        sum_length = 0\n",
    "        digits = 0\n",
    "        len_greater = 0\n",
    "        len_smaller = 0\n",
    "        for word in words:\n",
    "            sum_length += len(word)\n",
    "            if len(word) > max_len:\n",
    "                max_len = len(word)\n",
    "\n",
    "            if len(re.findall('\\d',word)) > 0:\n",
    "                digits += 1\n",
    "\n",
    "            if len(word) > 6:\n",
    "                len_greater += 1\n",
    "            elif len(word) < 3:\n",
    "                len_smaller += 1\n",
    "        \n",
    "        av_length.append(sum_length/len(words))\n",
    "        max_length.append(max_len)\n",
    "        num_numbers.append(digits)\n",
    "        num_greater.append(len_greater)\n",
    "        num_smaller.append(len_smaller)\n",
    "\n",
    "        # count stop-words \n",
    "        intersection_stop = list(set(stop_words) & set(words))\n",
    "        num_stop.append(len(intersection_stop))\n",
    "\n",
    "        # embeddings\n",
    "        for text in tweets:\n",
    "            aux_tweet = []\n",
    "            # clean each tweet\n",
    "            cleaned = clean_text(text,stop_words)\n",
    "            # compute vector for each word in the tweet using GloVe\n",
    "            for token in word_tokenize(cleaned,language='spanish',preserve_line=True):\n",
    "                vector = embedding_vectors.get(token)\n",
    "                if vector is not None:\n",
    "                    aux_tweet.append(vector)\n",
    "            if len(aux_tweet) != 0:\n",
    "                vec_tweet = np.asarray(aux_tweet).mean(0)\n",
    "                list_vector_tweet.append(vec_tweet)\n",
    "        # compute author's vector averaging tweets vectors\n",
    "        vec_author = np.asarray(list_vector_tweet).mean(0)\n",
    "        df.loc[len(df)] = vec_author\n",
    "\n",
    "gender_dict = {'female':0, 'male':1}\n",
    "\n",
    "age_dict = {}\n",
    "i = 0\n",
    "for age in data['age'].unique():\n",
    "    age_dict[age] = i\n",
    "    i += 1\n",
    "\n",
    "region_dict = {}\n",
    "i = 0\n",
    "for region in data['region'].unique():\n",
    "    region_dict[region] = i\n",
    "    i += 1\n",
    "    \n",
    "df['gender'] = data['gender'].map(gender_dict)\n",
    "df['age'] = data['age'].map(age_dict)\n",
    "df['region'] = data['region'].map(region_dict)\n",
    "df['mentions'] = num_mentions\n",
    "df['url'] = num_url\n",
    "df['hashtags'] = num_hashtags\n",
    "df['emojis'] = num_emojis\n",
    "df['characters'] = num_char\n",
    "df['capital_letters'] = num_capital\n",
    "df['punctuations'] = num_punctuation\n",
    "df['num_sentence'] = num_sentence\n",
    "df['av_sentence_par'] = av_sentence_par\n",
    "df['av_words_par'] = av_words_par\n",
    "df['av_char_par'] = av_char_par\n",
    "df['variation'] = variation\n",
    "df['num_det'] = num_det\n",
    "df['num_pre'] = num_pre\n",
    "df['num_sing'] = num_sing\n",
    "df['num_plural'] = num_plural\n",
    "df['num_adv'] = num_adv\n",
    "df['num_adj'] = num_adj\n",
    "df['num_prop'] = num_prop\n",
    "df['num_pronouns'] = num_pronouns\n",
    "df['num_past'] = num_past\n",
    "df['num_future'] = num_future\n",
    "df['num_conj'] = num_conj\n",
    "df['num_words'] = num_words\n",
    "df['num_pos_words'] = num_pos_words\n",
    "df['num_neg_words'] = num_neg_words\n",
    "df['num_unique'] = num_unique\n",
    "df['num_twice'] = num_twice\n",
    "df['av_length'] = av_length\n",
    "df['max_length'] = max_length\n",
    "df['num_numbers'] = num_numbers\n",
    "df['num_greater'] = num_greater\n",
    "df['num_smaller'] = num_smaller\n",
    "df['num_stop'] = num_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [i for i in range(300)]\n",
    "col_drop = [f for f in df.columns if f not in features]\n",
    "df.drop(col_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([              0,               1,               2,               3,\n",
       "                     4,               5,               6,               7,\n",
       "                     8,               9,\n",
       "       ...\n",
       "       'num_pos_words', 'num_neg_words',    'num_unique',     'num_twice',\n",
       "           'av_length',    'max_length',   'num_numbers',   'num_greater',\n",
       "         'num_smaller',      'num_stop'],\n",
       "      dtype='object', length=337)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URLS_RE = re.compile(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b')\n",
    "URLS_RE = re.compile(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*')\n",
    "\n",
    "LISTING_RE = re.compile(r'^(|[a-z]?|[0-9]{0,3})(\\-|\\.)+( |\\n)')\n",
    "\n",
    "def remove_urls(text):\n",
    "    return URLS_RE.sub('', text)\n",
    "\n",
    "def replace_multi_whitespaces(line):\n",
    "    return ' '.join(line.split())\n",
    "\n",
    "def remove_listing(line):\n",
    "    return LISTING_RE.sub('', line)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.replace('!','')\n",
    "    text = text.replace('\"','')\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ü\",\"u\"),\n",
    "        (\"ñ\",\"n\"),\n",
    "        (\"ç\",\"c\"),\n",
    "        (\"\\u2026\",\"...\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "\n",
    "def remove_stopwords(text,stop_words):\n",
    "    words = text.split(' ')\n",
    "    not_stop_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            not_stop_words.append(word)\n",
    "    return ' '.join(not_stop_words)\n",
    "      \n",
    "\n",
    "def clean_text(text,stop_words):\t\n",
    "\n",
    "    low = text.lower()\n",
    "    norm = normalize(low)\n",
    "    rem_u = remove_urls(norm)\n",
    "    rem_l = remove_listing(rem_u)\n",
    "    rem_w = replace_multi_whitespaces(rem_l)\n",
    "    rem_p = remove_punctuation(rem_w)\n",
    "    rem_s = remove_stopwords(rem_p,stop_words)\n",
    "    text_enc = rem_s.encode('ascii', 'ignore')\n",
    "\n",
    "    return text_enc.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded GloVe\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = {}\n",
    "with open('embeddings/glove-sbwc.i25.vec','r') as f:\n",
    "    first_line = f.readline().split(' ')\n",
    "    for line in f.readlines()[1:]:\n",
    "        row = line.split(' ')\n",
    "        # remove accents\n",
    "        word = normalize(row[0])\n",
    "        if word not in embedding_vectors.keys():\n",
    "            embedding_vectors[word] = [float(val) for val in row[1:]]\n",
    "    \n",
    "    num_words = int(first_line[0])\n",
    "    emb_dim = int(first_line[1])\n",
    "\n",
    "print('loaded GloVe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spanish stop words and remove accents (tweets dont have accents)\n",
    "stop_words_df = pd.read_csv('../spanish-stop-words.txt',header=None)\n",
    "stop_words = [normalize(w) for w in list(stop_words_df[0])] + ['q','ma']\n",
    "\n",
    "data = pd.read_excel('../cleaned_users.xlsx')\n",
    "username_list = data['username']\n",
    "\n",
    "df = pd.DataFrame(columns=[i for i in range(300)])\n",
    "\n",
    "list_author_vector = []\n",
    "for username in username_list:\n",
    "    list_vector_tweet = []\n",
    "    with open(f'../Cleaned Documents/{username}.txt','r') as f:\n",
    "        for text in f.readlines():\n",
    "            aux_tweet = []\n",
    "            # clean each tweet\n",
    "            cleaned = clean_text(text,stop_words)\n",
    "            # compute vector for each word in the tweet using GloVe\n",
    "            for token in word_tokenize(cleaned,language='spanish',preserve_line=True):\n",
    "                vector = embedding_vectors.get(token)\n",
    "                if vector is not None:\n",
    "                    aux_tweet.append(vector)\n",
    "\n",
    "            if len(aux_tweet) != 0:\n",
    "                vec_tweet = np.asarray(aux_tweet).mean(0)\n",
    "                list_vector_tweet.append(vec_tweet)\n",
    "        # compute author's vector averaging tweets vectors\n",
    "        vec_author = np.asarray(list_vector_tweet).mean(0)\n",
    "        df.loc[len(df)] = vec_author\n",
    "\n",
    "gender_dict = {'female':0, 'male':1}\n",
    "\n",
    "age_dict = {}\n",
    "i = 0\n",
    "for age in data['age'].unique():\n",
    "    age_dict[age] = i\n",
    "    i += 1\n",
    "\n",
    "region_dict = {}\n",
    "i = 0\n",
    "for region in data['region'].unique():\n",
    "    region_dict[region] = i\n",
    "    i += 1\n",
    "    \n",
    "df['gender'] = data['gender'].map(gender_dict)\n",
    "df['age'] = data['age'].map(age_dict)\n",
    "df['region'] = data['region'].map(region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.147999</td>\n",
       "      <td>0.159079</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>0.130502</td>\n",
       "      <td>0.126120</td>\n",
       "      <td>0.045860</td>\n",
       "      <td>0.045618</td>\n",
       "      <td>-0.096360</td>\n",
       "      <td>-0.030039</td>\n",
       "      <td>-0.144021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042676</td>\n",
       "      <td>-0.007225</td>\n",
       "      <td>-0.035417</td>\n",
       "      <td>-0.032703</td>\n",
       "      <td>-0.015888</td>\n",
       "      <td>0.060023</td>\n",
       "      <td>0.044763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013895</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.104365</td>\n",
       "      <td>-0.012638</td>\n",
       "      <td>0.032255</td>\n",
       "      <td>0.012641</td>\n",
       "      <td>-0.068697</td>\n",
       "      <td>-0.145879</td>\n",
       "      <td>-0.033884</td>\n",
       "      <td>-0.111700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.037748</td>\n",
       "      <td>-0.054684</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>0.113471</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.086022</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>-0.028159</td>\n",
       "      <td>-0.044578</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.052359</td>\n",
       "      <td>0.101972</td>\n",
       "      <td>-0.098933</td>\n",
       "      <td>0.060675</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044116</td>\n",
       "      <td>-0.041143</td>\n",
       "      <td>-0.026457</td>\n",
       "      <td>-0.108212</td>\n",
       "      <td>0.135126</td>\n",
       "      <td>0.038695</td>\n",
       "      <td>0.065699</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.152850</td>\n",
       "      <td>0.128347</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>0.163639</td>\n",
       "      <td>0.086703</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>-0.111563</td>\n",
       "      <td>-0.031908</td>\n",
       "      <td>-0.006062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>-0.054951</td>\n",
       "      <td>-0.107304</td>\n",
       "      <td>0.044327</td>\n",
       "      <td>-0.018754</td>\n",
       "      <td>0.034084</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.063309</td>\n",
       "      <td>0.107178</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.089620</td>\n",
       "      <td>0.151430</td>\n",
       "      <td>0.085237</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>-0.140060</td>\n",
       "      <td>-0.034566</td>\n",
       "      <td>-0.064955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083489</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>-0.020304</td>\n",
       "      <td>-0.082852</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>-0.008659</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.147999  0.159079  0.013434  0.130502  0.126120  0.045860  0.045618   \n",
       "1  0.013895  0.004696  0.104365 -0.012638  0.032255  0.012641 -0.068697   \n",
       "2 -0.086022  0.074902 -0.028159 -0.044578  0.119826  0.052359  0.101972   \n",
       "3 -0.152850  0.128347  0.006494  0.050184  0.163639  0.086703  0.029563   \n",
       "4 -0.063309  0.107178  0.020476  0.089620  0.151430  0.085237  0.009509   \n",
       "\n",
       "          7         8         9  ...       293       294       295       296  \\\n",
       "0 -0.096360 -0.030039 -0.144021  ...  0.042676 -0.007225 -0.035417 -0.032703   \n",
       "1 -0.145879 -0.033884 -0.111700  ...  0.002509  0.037748 -0.054684  0.039098   \n",
       "2 -0.098933  0.060675  0.003302  ... -0.044116 -0.041143 -0.026457 -0.108212   \n",
       "3 -0.111563 -0.031908 -0.006062  ...  0.018342  0.008454 -0.054951 -0.107304   \n",
       "4 -0.140060 -0.034566 -0.064955  ...  0.083489  0.031213 -0.020304 -0.082852   \n",
       "\n",
       "        297       298       299  gender  age  region  \n",
       "0 -0.015888  0.060023  0.044763       0    0       0  \n",
       "1  0.007610  0.113471  0.015578       1    1       0  \n",
       "2  0.135126  0.038695  0.065699       1    1       0  \n",
       "3  0.044327 -0.018754  0.034084       0    1       0  \n",
       "4 -0.005207  0.019539 -0.008659       1    2       0  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['region']\n",
    "df.drop(['gender','age','region'],axis=1,inplace=True)\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, target, stratify=target, shuffle=True,test_size=0.2,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC() \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunamancebo/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test,y_pred)\n",
    "prec = precision_score(y_test,y_pred,average='weighted')\n",
    "rec = recall_score(y_test,y_pred,average='weighted')\n",
    "f1 = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "with open('results.txt','a') as f:\n",
    "    f.write('\\t* Without tunning:\\n')\n",
    "    f.write(f'\\t\\t-> Accuracy: {acc}\\n')\n",
    "    f.write(f'\\t\\t-> Precision: {prec}\\n')\n",
    "    f.write(f'\\t\\t-> Recall: {rec}\\n')\n",
    "    f.write(f'\\t\\t-> F1-score: {f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000.0, 'gamma': 0.1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = list(['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "c = list([1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 1e2, 1e3, 1e4,1e5])\n",
    "gammas = list([0.1, 1, 10, 100])\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "param_grid = dict(kernel=kernels, C=c, gamma=gammas)\n",
    "grid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = grid.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "prec = precision_score(y_test,y_pred,average='weighted')\n",
    "rec = recall_score(y_test,y_pred,average='weighted')\n",
    "f1 = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "with open('results.txt','a') as f:\n",
    "    f.write('\\t* With tunning:\\n')\n",
    "    f.write(f'\\t\\t-> Accuracy: {acc}\\n')\n",
    "    f.write(f'\\t\\t-> Precision: {prec}\\n')\n",
    "    f.write(f'\\t\\t-> Recall: {rec}\\n')\n",
    "    f.write(f'\\t\\t-> F1-score: {f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
